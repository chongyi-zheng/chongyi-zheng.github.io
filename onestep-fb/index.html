<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Can We Really Learn One Representation to Optimize All Rewards? | Chongyi Zheng</title> <meta name="author" content="Chongyi Zheng"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/princeton.svg?2e8da7ab216364f60102077d93bdaaf9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="chongyi-zheng.github.io/onestep-fb/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Chongyi </span>Zheng</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="https://scholar.google.com/citations?user=bezWXYcAAAAJ" rel="external nofollow noopener" target="_blank">publications</a> </li> <li class="nav-item"><a class="nav-link" href="https://github.com/chongyi-zheng" title="Github" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a></li> <li class="nav-item"><a class="nav-link" href="https://twitter.com/chongyiz1" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a></li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <p class="post-description"></p> </header> <article> <style>body{padding-top:15px!important}header,.navbar,.site-header{margin-top:0!important;padding-top:0!important}.publication-links a.button,.publication-links a.button:visited{background-color:#363636!important;border-color:#363636!important;color:#fff!important;text-decoration:none!important}.publication-links a.button .icon,.publication-links a.button .icon i,.publication-links a.button span{color:inherit!important}.publication-links a.button:hover,.publication-links a.button:focus{background-color:#2b2b2b!important;border-color:#2b2b2b!important;color:#fff!important}</style> <meta charset="utf-8"> <meta name="description" content="Can We Really Learn One Representation to Optimize All Rewards?"> <meta name="keywords" content="One-Step Forward-Backward Representation Learning"> <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <title>Can We Really Learn One Representation to Optimize All Rewards?</title> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <style>.section.compact{padding-top:1.25rem;padding-bottom:1.25rem}.section.compact .title.is-3{margin-top:.1rem}.section.compact .content>:last-child,.section.compact .publication-image:last-child,.section.compact .video-container:last-child{margin-bottom:0}.publication-title{font-size:3rem;font-weight:bold}.publication-authors{margin-top:1rem}.author-block{display:inline-block;margin:0 .5rem}.publication-links{margin-top:1.5rem}.link-block{margin:.25rem}.content.has-text-justified{text-align:justify}.publication-image{margin:2rem 0}hr.rounded{border:2px solid #e5e5e5;border-radius:5px;margin:3rem 0}.hidden{display:none}.toggle-link{color:#1f77b4;cursor:pointer;display:inline-block;margin-bottom:1rem}.toggle-link:hover{color:#1f77b4}.table-image-container{margin:1.5rem 0}.table-image-container img{width:100%;height:auto}.charts-container{margin:1.5rem 0}.charts-container img{width:100%;height:auto}.caption-text{margin:1rem 0;font-style:italic}.benefits-list{margin:1rem 0 1rem 2rem}.benefits-list li{margin:.5rem 0}.task-selector{margin:2rem 0}.task-selector .button{margin:.25rem}.video-container{margin:1rem 0}.video-title{font-size:1.5rem;font-weight:bold;margin-bottom:1rem}.video-wrapper{display:flex;justify-content:center;gap:1rem;flex-wrap:wrap}.video-item{flex:1;min-width:250px;max-width:2048px}.video-item video{width:100%;height:auto;border-radius:8px}.video-label{text-align:center;margin-top:.5rem;font-weight:bold}.video-grid{display:grid;grid-template-columns:repeat(5,1fr);gap:12px}.video-grid>.video{grid-column:1 / -1;width:100%;display:block;border-radius:8px}.caption-grid{grid-column:1 / -1;display:grid;grid-template-columns:repeat(5,1fr);gap:12px;font-size:.95rem;line-height:1.4}.proposition-block{background-color:#f0f7ff;border-left:6px solid #2d5a9e;border-radius:4px;padding:24px;margin:30px 0;box-shadow:0 2px 5px rgba(0,0,0,0.05)}.proposition-header{color:#2d5a9e;font-weight:bold;font-variant:small-caps;font-size:1.1em;margin-bottom:12px;display:block}.proposition-block ol{margin-top:15px;padding-left:25px}.proposition-block li{margin-bottom:10px}@media(max-width:1024px){.video-grid,.caption-grid{grid-template-columns:repeat(3,1fr)}}@media(max-width:640px){.video-grid,.caption-grid{grid-template-columns:1fr}}</style> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title">Can We Really Learn One Representation to Optimize All Rewards?</h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://chongyi-zheng.github.io/" rel="external nofollow noopener" target="_blank">Chongyi Zheng*</a><sup>1</sup>  </span> <span class="author-block"> <a href="http://www.linkedin.com/in/royina-karegoudra-jayanth-3a7bb9194" rel="external nofollow noopener" target="_blank">Royina Karegoudra Jayanth*</a><sup>1</sup>  </span> <span class="author-block"> <a href="https://ben-eysenbach.github.io/" rel="external nofollow noopener" target="_blank">Benjamin Eysenbach</a><sup>1</sup> </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"> <sup>1</sup>Princeton University </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"> (*Equal contribution) </span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://arxiv.org/abs/2602.11399" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"><i class="fas fa-file-pdf"></i></span> <span>Paper</span> </a> <a href="https://github.com/chongyi-zheng/onestep-fb" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"><i class="fas fa-code"></i></span> <span>Code</span> </a> <a href="https://x.com/chongyiz1/status/2022359479652950453?s=20" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"><i class="fab fa-twitter"></i></span> <span>Thread</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/onestep_fb.svg" alt="Teaser image" style="width: 90%; height: auto;"> </div> <h3 class="title is-3">Overview</h3> <div class="content has-text-justified"> <p> Modern machine learning has moved towards leveraging large models as priors for downstream tasks. For example, we have large language models (LLMs) in natural language processing and large-scale self-supervised representation learning in computer vision. <i>How to build large models as good priors for solving reinforcement learning (RL) problems?</i> One promising direction is to learn a prior over the policies of some yet-to-be-determined tasks: prefetch as much computation as possible before a specific reward function is known. </p> <p> Recent work (<a href="https://arxiv.org/abs/2103.07945" rel="external nofollow noopener" target="_blank">forward-backward (FB) representation learning</a>) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over <i>arbitrary</i> rewards without further fine-tuning. Conceptually, this idea resembles the <a href="https://arxiv.org/pdf/2005.14165" rel="external nofollow noopener" target="_blank">in-context learning</a> in LLMs. However, this formulation results in a circular dependency between learned representations and policies (see the figure above), incurring optimization challenges. In this work, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. </p> <p> Our analysis suggests a simplified unsupervised pre-training method for RL that breaks the circular dependency (see the figure above): <b>one-step forward-backward representation learning (one-step FB)</b>. Instead of enabling optimal control, one-step FB performs one step of policy improvement. </p> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h3 class="title is-3">Demystifying Forward-Backward Representation Learning</h3> <div class="content has-text-justified"> <p> Forward-backward representation learning factorizes successor measures of the current policies into bilinear representations, and uses those representations to acquire new policies. <a href="https://arxiv.org/abs/2103.07945" rel="external nofollow noopener" target="_blank">Touati &amp; Ollivier</a> implicitly <i>assumes</i> that, in discrete controlled Markov processes (CMPs; Markov decision processes without a reward function), a ground-truth (oracle) factorization exists and the practical FB algorithm can converge to it. We want to both theoretically and empirically understand whether these assumptions hold for FB. Therefore, we aim to answer the following questions in our analysis. </p> <p> <b>When do the ground-truth FB representations exist?</b> In discrete CMPs, we find four necessary conditions for the existence of ground-truth FB representations: <a href="#" class="toggle-link" id="toggleExistenceConditions"> Click to see the four necessary conditions. </a> </p> <div id="existenceConditionsContainer" class="hidden"> <ol class="benefits-list"> <li> <i>representation dimension \( d \): \( d \geq \left| \mathcal{S} \times \mathcal{A} \right| \).</i> </li> <li> <i>rank of the ground-truth forward representation matrix \( F_{\mathcal{Z}}^{\star} \): \( \left| \mathcal{S} \times \mathcal{A} \right| \leq \text{rank}(F_{\mathcal{Z}}^{\star}) \leq d \).</i> </li> <li> <i>rank of the ground-truth backward representation matrix \( B^{\star} \): \( \text{rank}(B^{\star}) = d \).</i> </li> <li> <i>relationship between the ground-truth forward-backward representation matrices and successor measures \( M^{\pi}( a \mid s, z) \): $$ B^{\star} = F^{\star +}_{z_1} M^{\pi(a \mid s, z_1)} / \rho = \dots = F^{\star +}_{z_{|\mathcal{Z}|}} M^{\pi( a \mid s, z_{|\mathcal{Z}|})} / \rho, $$ where \(X^{+}\) denotes the pseudoinverse of the matrix \(X\). </i> </li> </ol> </div> <p> <b>What does the FB representation objective minimize?</b> We interpret the representation objective in FB as a temporal-difference (TD) variant of the least-squares importance fitting (LSIF) loss, drawing a connection to <a href="https://link.springer.com/chapter/10.1007/11564096_32" rel="external nofollow noopener" target="_blank">fitted Q-evaluation</a> (FQE). FQE performs approximate value iteration, which (approximately) enjoys convergence guarantee. This connection motivates us to study whether FB admits a similar convergence in practice. </p> <p> <b>Does the practical FB algorithm converge to ground-truth representations?</b> We define a new FB Bellman operator to answer this question. Unfortunately, the FB Bellman operator is <b>not</b> a \(\gamma\)-contraction, suggesting that the <a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" rel="external nofollow noopener" target="_blank">Banach fixed-point theorem</a> fails to apply. </p> <p> In theory, whether the FB algorithm converges to any ground-truth representations remains an open problem. The key challenge comes from the circular dependency between the FB representations and the policies. In practice, we will use didactic experiments to demonstrate the failure convergence of FB. </p> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full"> <h3 class="title is-3">A Simplified Algorithm for Unsupervised Pre-training in RL</h3> <div class="content has-text-justified"> <p> Our understanding of FB reveals that its inherent circular dependency incurs optimization challenges and unclear learning behaviors. To break the circular dependency, we propose a simplified pre-training method for RL called one-step forward-backward (one-step FB) representation learning. </p> <ul class="benefits-list"> <li>Instead of learning FB representations for the current policy, one-step FB learns representations for a <i>fixed</i> behavioral policy \( \pi_{\beta} \).</li> <li>Unlike FB, which argues to prefetch optimal controls for arbitrary reward functions, one-step FB performs policy adaptation via one step of policy improvement.</li> <li>Empirically, we find that one-step FB enjoys clear convergence in didactic discrete CMPs and achieves comparable zero-shot performance against FB on standard offline RL benchmarks.</li> <li>Starting from the existing FB algorithm, implementing our method requires making two simple changes.</li> <ol class="benefits-list"> <li>Remove the latent variable from the input to the forward representation.</li> <li>In the representation loss, sample the next action from the dataset instead of the target policy.</li> </ol> </ul> <a href="#" class="toggle-link" id="toggleAlgorithm">Click to see the complete algorithm.</a> <div id="algorithmContainer" class="hidden"> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/onestep_fb_algo.jpeg" alt="One-step FB algorithm"> </figure> </div> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h3 class="title is-3">Experiments on Standard Benchmarks</h3> <h4 class="title is-4">Domains</h4> <div style="display:flex; justify-content:center; gap:1rem; margin:1.5em 0;"> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/walker.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">walker</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/cheetah.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">cheetah</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/quadruped.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">quadruped</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/jaco.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">jaco</figcaption> </figure> </div> <div style="display:flex; justify-content:center; gap:1rem; margin:1.5em 0;"> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/antmaze-large-navigate-v0_data.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">antmaze large</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/antmaze-teleport-navigate-v0_data.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">antmaze teleport</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/cube-single-v0_task4.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">cube single</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/scene-v0_task5.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">scene</figcaption> </figure> </div> <div class="content has-text-justified"> We use domains from the standard offline RL benchmarks to compare the performance of one-step FB against \(5\) prior unsupervised pre-training methods for RL. <ul class="benefits-list"> <li>We select a set of \(4\) state-based domains from the <a href="https://arxiv.org/abs/2201.13425" rel="external nofollow noopener" target="_blank">ExORL</a> benchmark.</li> <li>We also select a set of \( 6 \) state- and image-based domains from the <a href="https://arxiv.org/abs/2410.20092" rel="external nofollow noopener" target="_blank">OGBench</a> benchmark.</li> <li>While pre-training a set of policies using each algorithm, we measure the performance of zero-shot policy adaptation to downstream tasks (\(16\) tasks from ExORL and \(30\) tasks from OGBench).</li> </ul> </div> <h4 class="title is-4">Offline zero-shot RL</h4> <div class="content has-text-justified"> <a href="#" class="toggle-link" id="toggleTable"> Click to see the full table (<span id="taskCount">46</span> tasks) </a> <div id="tableContainer" class="hidden"> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/zero_shot_rl_eval.png" alt="Offline zero-shot RL evaluation"> </figure> </div> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/zero_shot_rl_eval_agg.png" alt="Offline zero-shot RL evaluations aggregated across domains"> </figure> <ul class="benefits-list"> <li>One-step FB achieves the best or near-best performance on \( 6 \) out of \( 10 \) domains.</li> <li>Compared with FB, one-step FB achieves \(+1.4 \times\) improvement on average.</li> <li>One-step FB is able to outperform prior methods by \(20\%\) using RGB images directly.</li> </ul> </div> <h4 class="title is-4">Offline-to-online fine-tuning</h4> <div class="content has-text-justified"> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/finetuning_lcs.svg" alt="Offline-to-online fine-tuning learning curves"> </figure> <ul class="benefits-list"> <li>After offline unsupervised pre-training, we conduct online fine-tuning of the policies pre-trained by different methods using the same off-the-shelf RL algorithm (TD3).</li> <li>One-step FB continues to provide higher sample efficiency (\(+40\%\) on average) during fine-tuning, as compared with the original FB method.</li> <li>The fine-tuned policies reach the asymptotic performance of TD3 at the end of training.</li> </ul> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h3 class="title is-3">The Key Components of One-Step FB</h3> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/lambda_ortho_ablations.svg" alt="Orthonormalization regularization ablation" style="width: 100%; height: auto;"> <figcaption class="content has-text-justified" style="font-style: italic;"> Using an appropriate value of the orthonormalization regularization strength \( \lambda_{\text{ortho}} \) is key to the performance of one-step FB. </figcaption> </div> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/tau_reward_ablations.svg" alt="Reward temperature ablation" style="width: 100%; height: auto;"> <figcaption class="content has-text-justified" style="font-style: italic;"> During zero-shot adaptation, we reweight the reward function using a softmax weight with temperature \( \tau_{\text{reward}} \). One-step FB is less sensitive to the choice of \( \tau_{\text{reward}} \) on different domains. </figcaption> </div> </div> </div> </div> </section> <section class="section compact" id="BibTeX"> <div class="container is-max-desktop content"> <h3 class="title is-3">BibTeX</h3> <pre><code>@article{zheng2026can,
  title={Can We Really Learn One Representation to Optimize All Rewards?}, 
  author={Zheng, Chongyi and Jayanth, Royina Karegoudra and Eysenbach, Benjamin},
  journal={arXiv preprint arXiv:2602.11399},
  year={2026},
}</code></pre> </div> </section> <script>function changeTask(e,t,n,i){const d=document.getElementById("video-title"),o=document.getElementById("task-video-player-1"),s=document.getElementById("task-video-player-2"),l=document.getElementById("task-video-player-3"),c=document.getElementById("task-video-item-2");d.textContent=e.charAt(0).toUpperCase()+e.slice(1),o.src=t,o.load(),n?(s.src=n,s.load(),c.style.display="block"):c.style.display="none",l.src=i,l.load()}document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("toggleExistenceConditions"),t=document.getElementById("existenceConditionsContainer");if(e&&t){let n=!1;e.addEventListener("click",function(i){i.preventDefault(),n=!n,n?(t.classList.remove("hidden"),e.innerHTML="Click to hide the four necessary conditions."):(t.classList.add("hidden"),e.innerHTML="Click to see the four necessary conditions.")})}const n=document.getElementById("toggleAlgorithm"),i=document.getElementById("algorithmContainer");if(n&&i){let e=!1;n.addEventListener("click",function(t){t.preventDefault(),e=!e,e?(i.classList.remove("hidden"),n.innerHTML="Click to hide the complete algorithm."):(i.classList.add("hidden"),n.innerHTML="Click to see the complete algorithm.")})}const d=document.getElementById("toggle5StateCMP"),o=document.getElementById("5StateCMPContainer");if(d&&o){let e=!1;d.addEventListener("click",function(t){t.preventDefault(),e=!e,e?(o.classList.remove("hidden"),d.innerHTML="Click to hide didactic experiments on another CMP"):(o.classList.add("hidden"),d.innerHTML="Click to see didactic experiments on another CMP")})}const s=document.getElementById("toggleTable"),l=document.getElementById("tableContainer");if(s&&l){let e=!1;s.addEventListener("click",function(t){t.preventDefault(),e=!e,e?(l.classList.remove("hidden"),s.innerHTML='Click to hide the full table (<span id="taskCount">46</span> tasks)'):(l.classList.add("hidden"),s.innerHTML='Click to see the full table (<span id="taskCount">46</span> tasks)')})}});</script> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2026 Chongyi Zheng. <span style="float:right;"> Last updated: February 13, 2026. </span> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",macros:{textsc:["\\style{font-variant-caps: small-caps}{\\text{#1}}",1]}}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>