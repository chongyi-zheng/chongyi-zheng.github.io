<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Can We Really Learn One Representation to Optimize All Rewards? | Chongyi Zheng</title> <meta name="author" content="Chongyi Zheng"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/princeton.svg?2e8da7ab216364f60102077d93bdaaf9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="chongyi-zheng.github.io/onestep-fb/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Chongyi </span>Zheng</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item"> <a class="nav-link" href="https://scholar.google.com/citations?user=bezWXYcAAAAJ" rel="external nofollow noopener" target="_blank">publications</a> </li> <li class="nav-item"><a class="nav-link" href="https://github.com/chongyi-zheng" title="Github" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a></li> <li class="nav-item"><a class="nav-link" href="https://twitter.com/chongyiz1" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a></li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <p class="post-description"></p> </header> <article> <style>body{padding-top:15px!important}header,.navbar,.site-header{margin-top:0!important;padding-top:0!important}.publication-links a.button,.publication-links a.button:visited{background-color:#363636!important;border-color:#363636!important;color:#fff!important;text-decoration:none!important}.publication-links a.button .icon,.publication-links a.button .icon i,.publication-links a.button span{color:inherit!important}.publication-links a.button:hover,.publication-links a.button:focus{background-color:#2b2b2b!important;border-color:#2b2b2b!important;color:#fff!important}</style> <meta charset="utf-8" name="description" content="Can We Really Learn One Representation to Optimize All Rewards?"> <meta name="keywords" content="One-Step Forward-Backward Representation Learning"> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <title>Can We Really Learn One Representation to Optimize All Rewards?</title> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <style>.section.compact{padding-top:1.25rem;padding-bottom:1.25rem}.section.compact .title.is-3{margin-top:.1rem}.section.compact .content>:last-child,.section.compact .publication-image:last-child,.section.compact .video-container:last-child{margin-bottom:0}.publication-title{font-size:3rem;font-weight:bold}.publication-authors{margin-top:1rem}.author-block{display:inline-block;margin:0 .5rem}.publication-links{margin-top:1.5rem}.link-block{margin:.25rem}.content.has-text-justified{text-align:justify}.publication-image{margin:2rem 0}hr.rounded{border:2px solid #e5e5e5;border-radius:5px;margin:3rem 0}.hidden{display:none}.toggle-link{color:#1f77b4;cursor:pointer;display:inline-block;margin-bottom:1rem}.toggle-link:hover{color:#1f77b4}.table-image-container{margin:1.5rem 0}.table-image-container img{width:100%;height:auto}.charts-container{margin:1.5rem 0}.charts-container img{width:100%;height:auto}.caption-text{margin:1rem 0;font-style:italic}.benefits-list{margin:1rem 0 1rem 2rem}.benefits-list li{margin:.5rem 0}.task-selector{margin:2rem 0}.task-selector .button{margin:.25rem}.video-container{margin:1rem 0}.video-title{font-size:1.5rem;font-weight:bold;margin-bottom:1rem}.video-wrapper{display:flex;justify-content:center;gap:1rem;flex-wrap:wrap}.video-item{flex:1;min-width:250px;max-width:2048px}.video-item video{width:100%;height:auto;border-radius:8px}.video-label{text-align:center;margin-top:.5rem;font-weight:bold}.video-grid{display:grid;grid-template-columns:repeat(5,1fr);gap:12px}.video-grid>.video{grid-column:1 / -1;width:100%;display:block;border-radius:8px}.caption-grid{grid-column:1 / -1;display:grid;grid-template-columns:repeat(5,1fr);gap:12px;font-size:.95rem;line-height:1.4}.proposition-block{background-color:#f0f7ff;border-left:6px solid #2d5a9e;border-radius:4px;padding:24px;margin:30px 0;box-shadow:0 2px 5px rgba(0,0,0,0.05)}.proposition-header{color:#2d5a9e;font-weight:bold;font-variant:small-caps;font-size:1.1em;margin-bottom:12px;display:block}.proposition-block ol{margin-top:15px;padding-left:25px}.proposition-block li{margin-bottom:10px}@media(max-width:1024px){.video-grid,.caption-grid{grid-template-columns:repeat(3,1fr)}}@media(max-width:640px){.video-grid,.caption-grid{grid-template-columns:1fr}}</style> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title">Can We Really Learn One Representation to Optimize All Rewards?</h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://chongyi-zheng.github.io/" rel="external nofollow noopener" target="_blank">Chongyi Zheng*</a><sup>1</sup>  </span> <span class="author-block"> <a href="http://www.linkedin.com/in/royina-karegoudra-jayanth-3a7bb9194" rel="external nofollow noopener" target="_blank">Royina Karegoudra Jayanth*</a><sup>1</sup>  </span> <span class="author-block"> <a href="https://ben-eysenbach.github.io/" rel="external nofollow noopener" target="_blank">Benjamin Eysenbach</a><sup>1</sup> </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"> <sup>1</sup>Princeton University </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"> (*Equal contribution) </span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://arxiv.org/abs/xx" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"><i class="fas fa-file-pdf"></i></span> <span>Paper</span> </a> <a href="https://github.com/chongyi-zheng/onestep-fb" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"><i class="fas fa-code"></i></span> <span>Code</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/onestep_fb.svg" alt="Teaser image" style="width: 90%; height: auto;"> </div> <h3 class="title is-3">Overview</h3> <div class="content has-text-justified"> <p> <i>How to leverage the right form of prior to pre-train large models for solving reinforcement learning (RL) problems?</i> One promising direction is to learn a prior over the policies of some yet-to-be-determined tasks: prefetch as much computation as possible before a specific reward function is known. </p> <p> Recent work (<a href="https://arxiv.org/abs/2103.07945" rel="external nofollow noopener" target="_blank">forward-backward (FB) representation learning</a>) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. Our work demystifies FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. </p> <p> Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement: <b>one-step forward-backward representation learning (one-step FB)</b>. </p> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h3 class="title is-3">Theoretical Findings</h3> <div class="content has-text-justified"> <p> We aim to answer the following questions in our theoretical analysis of FB: </p> <p> <b>When do the ground-truth FB representations exist?</b> In discrete controlled Markov processes (CMPs), we find four necessary conditions for the existence of ground-truth FB representations: <a href="#" class="toggle-link" id="toggleExistenceConditions"> Click to see the four necessary conditions. </a> </p> <div id="existenceConditionsContainer" class="hidden"> <ol class="benefits-list"> <li> <i>representation dimension \( d \): \( d \geq \left| \mathcal{S} \times \mathcal{A} \right| \).</i> </li> <li> <i>rank of the ground-truth forward representation matrix \( F_{\mathcal{Z}}^{\star} \): \( \left| \mathcal{S} \times \mathcal{A} \right| \leq \text{rank}(F_{\mathcal{Z}}^{\star}) \leq d \).</i> </li> <li> <i>rank of the ground-truth backward representation matrix \( B^{\star} \): \( \text{rank}(B^{\star}) = d \).</i> </li> <li> <i>relationship between the ground-truth forward-backward representation matrices and successor measures \( M^{\pi}({\color{gray} a} \mid {\color{gray} s}, z) \): $$ B^{\star} = F^{\star +}_{z_1} M^{\pi({\color{gray} a} \mid {\color{gray} s}, z_1)} / \rho = \dots = F^{\star +}_{z_{|\mathcal{Z}|}} M^{\pi({\color{gray} a} \mid {\color{gray} s}, z_{|\mathcal{Z}|})} / \rho, $$ where \( {\color{gray} X}^{+} \) denotes the pseudoinverse of the matrix \({\color{gray} X} \). </i> </li> </ol> </div> <p> <b>What does the FB representation objective minimize?</b> We interpret the representation objective in FB as a temporal-difference (TD) variant of the least-squares importance fitting (LSIF) loss. This interpretation draws a connection with <a href="https://link.springer.com/chapter/10.1007/11564096_32" rel="external nofollow noopener" target="_blank">fitted Q-evaluation</a>. </p> <p> <b>Does the practical FB algorithm converge to ground-truth representations?</b> Our analysis suggests that, in theory, whether the FB algorithm converges to any ground-truth representations remains an open problem. The key challenge comes from the circular dependency between the FB representations and the policies (see the figure above). In practice, we will use didactic experiments to demonstrate the failure convergence of FB. </p> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full"> <h3 class="title is-3">Our Simplified Algorithm</h3> <div class="content has-text-justified"> <p> Our understanding of FB suggests a simplified pre-training method for RL called one-step forward-backward (one-step FB) representation learning. </p> <ul class="benefits-list"> <li>One-step FB breaks the circular dependency by learning representations for a <i>fixed</i> behavioral policy (see the figure above).</li> <li>One-step FB performs policy adaptation via one step of policy improvement.</li> <li>Starting from the existing FB algorithm, implementing our method requires making two simple changes.</li> </ul> <a href="#" class="toggle-link" id="toggleAlgorithm">Click to see the complete algorithm.</a> <div id="algorithmContainer" class="hidden"> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/onestep_fb_algo.jpeg" alt="One-step FB algorithm"> </figure> </div> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h3 class="title is-3">Didactic Experiments</h3> <div class="content has-text-justified"> <div class="content is-centered"> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/3_state_cmp.svg" alt="3 State CMP" style="width: 40%; height: auto;"> <figcaption class="content has-text-justified" style="font-style: italic;"> <b>The three-state CMP.</b> Agents start from state \(s_0\) and take action \(a_i\) (\(i = 0, 1, 2\)) to deterministically transit into state \(s_i\). States \(s_1\) and \(s_2\) are both absorbing states. </figcaption> </div> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/3_state_cmp_lcs.svg" alt="3 State CMP Results" style="width: 120%; height: auto;"> </div> <p> We will track several metrics with the aim of answering the following questions: </p> <ol class="benefits-list"> <li> <i>Do the learned representations accurately reflect the successor measure ratio?</i> \( M^{\pi} / \rho \) and \( M^{\pi_{\beta}} / \rho \) prediction errors.</li> <li> <i>Do the learned representations accurately reflect the ground-truth Q values?</i> \( Q^{\star} \) and \( Q^{\pi_{\beta}} \) prediction errors.</li> <li> <i>How similar are the learned policies to the ground-truth policies?</i> forward KL divergence (\( \pi^{\star} \)) and forward KL divergence (\( \text{argmax}_a Q^{\pi_{\beta}} \)).</li> <li> <i>Do the predicted Q-value satisfy the equivariance property of universal value functions?</i> \( \hat{Q} \) equivariance errors.</li> </ol> <p> Results: </p> <ul class="benefits-list"> <li> <i>(Left)</i> After training for \( 10^{5} \) gradient steps, FB fails to converge to ground-truth FB representations.</li> <li> <i>(Right)</i> Given a fixed policy, one-step FB exactly fits the ground-truth one-step FB representations.</li> </ul> <a href="#" class="toggle-link" id="toggle5StateCMP"> Click to see didactic experiments on another CMP </a> <div id="5StateCMPContainer" class="hidden"> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/5_state_cmp.svg" alt="5 State CMP" style="width: 50%; height: auto;"> <figcaption class="content has-text-justified" style="font-style: italic;"> <b>The five-state circular CMP.</b> Agents start from state \(s_0\) and take action \(a_i\) (\(i = 0, 1, 2\)). At every state \(s_i\), choosing \(a_0\) deterministically transits to the next state \( s_{(i + 1) \mod 5} \), forming circular transitions. At every state \( s_i \), choosing action \( a_1 \) transits to state \( s_{(i - 1) \mod 5} \) with a proprobability \( 0.7 \) and stays in the same state with a probability of \( 0.3 \), forming the stochastic transitions. </figcaption> </div> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/5_state_cmp_lcs.svg" alt="5 State CMP Results" style="width: 120%; height: auto;"> <figcaption class="content has-text-justified" style="font-style: italic;"> <b>(Left)</b> After training for \( 10^5 \) gradient steps, FB fails to converge to a pair of ground-truth FB representations. <b>(Right)</b> Given a fixed policy, one-step FB exactly fits the ground-truth one-step FB representations within \( 4 \times 10^4 \) gradient steps. These observations are consistent with our analysis on the three-state CMP. </figcaption> </div> </div> </div> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h3 class="title is-3">Experiments on Standard Benchmarks</h3> <h4 class="title is-4">Domains</h4> <div style="display:flex; justify-content:center; gap:1rem; margin:1.5em 0;"> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/walker.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">walker</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/cheetah.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">cheetah</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/quadruped.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">quadruped</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/jaco.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">jaco</figcaption> </figure> </div> <div style="display:flex; justify-content:center; gap:1rem; margin:1.5em 0;"> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/antmaze-large-navigate-v0_data.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">antmaze large</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/antmaze-teleport-navigate-v0_data.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">antmaze teleport</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/cube-single-v0_task4.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">cube single</figcaption> </figure> <figure class="table-image-container" style="margin:0; text-align:center; flex:1;"> <video autoplay loop muted playsinline style="width:100%; border-radius:4px;"> <source src="../assets/papers/onestep_fb/videos/scene-v0_task5.mp4" type="video/mp4"> Your browser doesn’t support the video tag. </source></video> <figcaption style="font-size:1.2rem; font-style: italic;">scene</figcaption> </figure> </div> <h4 class="title is-4">Offline zero-shot RL</h4> <div class="content has-text-justified"> <a href="#" class="toggle-link" id="toggleTable"> Click to see the full table (<span id="taskCount">46</span> tasks) </a> <div id="tableContainer" class="hidden"> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/zero_shot_rl_eval.png" alt="Offline zero-shot RL evaluation"> </figure> </div> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/zero_shot_rl_eval_agg.png" alt="Offline zero-shot RL evaluations aggregated across domains"> </figure> <ul class="benefits-list"> <li>One-step FB achieves the best or near-best performance on \( 6 \) out of \( 10 \) domains.</li> <li>Compared with FB, one-step FB achieves \(+1.4 \times\) improvement on average.</li> <li>One-step FB is able to outperform prior methods by \(20\%\) using RGB images directly.</li> </ul> </div> <h4 class="title is-4">Offline-to-online fine-tuning</h4> <div class="content has-text-justified"> <figure class="table-image-container"> <img src="../assets/papers/onestep_fb/images/finetuning_lcs.svg" alt="Offline-to-online fine-tuning learning curves"> </figure> <ul class="benefits-list"> <li>After offline unsupervised pre-training, we conduct online fine-tuning on various methods using the same off-the-shelf RL algorithm (TD3).</li> <li>One-step FB continues to provide higher sample efficiency (\(+40\%\) on average) during fine-tuning, as compared with the original FB method.</li> <li>The fine-tuned policies reach the asymptotic performance of TD3 at the end of training.</li> </ul> </div> </div> </div> </div> </section> <section class="section compact"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h3 class="title is-3">The Key Components of One-Step FB</h3> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/lambda_ortho_ablations.svg" alt="Orthonormalization regularization ablation" style="width: 100%; height: auto;"> <figcaption class="content has-text-justified" style="font-style: italic;"> Using an appropriate value of the orthonormalization regularization strength \( \lambda_{\text{ortho}} \) is key to the performance of one-step FB. </figcaption> </div> <div class="table-image-container" style="width:100%; text-align: center;"> <img src="../assets/papers/onestep_fb/images/tau_reward_ablations.svg" alt="Reward temperature ablation" style="width: 100%; height: auto;"> <figcaption class="content has-text-justified" style="font-style: italic;"> During zero-shot adaptation, we reweight the reward function using a softmax weight with temperature \( \tau_{\text{reward}} \). One-step FB is less sensitive to the choice of \( \tau_{\text{reward}} \) on different domains. </figcaption> </div> </div> </div> </div> </section> <section class="section compact" id="BibTeX"> <div class="container is-max-desktop content"> <h3 class="title is-3">BibTeX</h3> <pre><code>@article{zheng2026can,
  title={Can We Really Learn One Representation to Optimize All Rewards?}, 
  author={Zheng, Chongyi and Jayanth, Royina Karegoudra and Eysenbach, Benjamin},
  journal={arXiv preprint arXiv:xx},
  year={2026},
}</code></pre> </div> </section> <script>function changeTask(e,t,n,i){const d=document.getElementById("video-title"),o=document.getElementById("task-video-player-1"),s=document.getElementById("task-video-player-2"),c=document.getElementById("task-video-player-3"),l=document.getElementById("task-video-item-2");d.textContent=e.charAt(0).toUpperCase()+e.slice(1),o.src=t,o.load(),n?(s.src=n,s.load(),l.style.display="block"):l.style.display="none",c.src=i,c.load()}document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("toggleExistenceConditions"),t=document.getElementById("existenceConditionsContainer");if(e&&t){let n=!1;e.addEventListener("click",function(i){i.preventDefault(),n=!n,n?(t.classList.remove("hidden"),e.innerHTML="Click to hide the four necessary conditions."):(t.classList.add("hidden"),e.innerHTML="Click to see the four necessary conditions.")})}const n=document.getElementById("toggleAlgorithm"),i=document.getElementById("algorithmContainer");if(n&&i){let e=!1;n.addEventListener("click",function(t){t.preventDefault(),e=!e,e?(i.classList.remove("hidden"),n.innerHTML="Click to hide the four necessary conditions."):(i.classList.add("hidden"),n.innerHTML="Click to see the four necessary conditions.")})}const d=document.getElementById("toggle5StateCMP"),o=document.getElementById("5StateCMPContainer");if(d&&o){let e=!1;d.addEventListener("click",function(t){t.preventDefault(),e=!e,e?(o.classList.remove("hidden"),d.innerHTML="Click to hide didactic experiments on another CMP"):(o.classList.add("hidden"),d.innerHTML="Click to see didactic experiments on another CMP")})}const s=document.getElementById("toggleTable"),c=document.getElementById("tableContainer");if(s&&c){let e=!1;s.addEventListener("click",function(t){t.preventDefault(),e=!e,e?(c.classList.remove("hidden"),s.innerHTML='Click to hide the full table (<span id="taskCount">62</span> tasks)'):(c.classList.add("hidden"),s.innerHTML='Click to see the full table (<span id="taskCount">62</span> tasks)')})}});</script> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2026 Chongyi Zheng. <span style="float:right;"> Last updated: February 12, 2026. </span> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",macros:{textsc:["\\style{font-variant-caps: small-caps}{\\text{#1}}",1]}}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>